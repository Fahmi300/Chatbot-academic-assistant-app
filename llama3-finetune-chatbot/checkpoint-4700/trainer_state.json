{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.891202498698594,
  "eval_steps": 500,
  "global_step": 4700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010411244143675169,
      "grad_norm": 2.24688982963562,
      "learning_rate": 0.00019962500000000001,
      "loss": 2.2626,
      "step": 10
    },
    {
      "epoch": 0.020822488287350338,
      "grad_norm": 1.1514946222305298,
      "learning_rate": 0.00019920833333333336,
      "loss": 1.1272,
      "step": 20
    },
    {
      "epoch": 0.031233732431025507,
      "grad_norm": 1.1426711082458496,
      "learning_rate": 0.0001987916666666667,
      "loss": 0.9767,
      "step": 30
    },
    {
      "epoch": 0.041644976574700676,
      "grad_norm": 1.003821849822998,
      "learning_rate": 0.000198375,
      "loss": 1.0713,
      "step": 40
    },
    {
      "epoch": 0.052056220718375845,
      "grad_norm": 1.028425931930542,
      "learning_rate": 0.00019795833333333332,
      "loss": 0.9076,
      "step": 50
    },
    {
      "epoch": 0.062467464862051014,
      "grad_norm": 1.0619416236877441,
      "learning_rate": 0.00019754166666666667,
      "loss": 0.9445,
      "step": 60
    },
    {
      "epoch": 0.07287870900572618,
      "grad_norm": 1.1880524158477783,
      "learning_rate": 0.000197125,
      "loss": 0.956,
      "step": 70
    },
    {
      "epoch": 0.08328995314940135,
      "grad_norm": 0.8596462607383728,
      "learning_rate": 0.00019670833333333335,
      "loss": 0.9192,
      "step": 80
    },
    {
      "epoch": 0.09370119729307652,
      "grad_norm": 1.7499964237213135,
      "learning_rate": 0.00019629166666666666,
      "loss": 0.908,
      "step": 90
    },
    {
      "epoch": 0.10411244143675169,
      "grad_norm": 1.0599712133407593,
      "learning_rate": 0.000195875,
      "loss": 0.7925,
      "step": 100
    },
    {
      "epoch": 0.11452368558042686,
      "grad_norm": 1.0536097288131714,
      "learning_rate": 0.00019545833333333335,
      "loss": 0.8039,
      "step": 110
    },
    {
      "epoch": 0.12493492972410203,
      "grad_norm": 1.2288542985916138,
      "learning_rate": 0.0001950416666666667,
      "loss": 0.8096,
      "step": 120
    },
    {
      "epoch": 0.1353461738677772,
      "grad_norm": 1.3986684083938599,
      "learning_rate": 0.000194625,
      "loss": 0.8555,
      "step": 130
    },
    {
      "epoch": 0.14575741801145237,
      "grad_norm": 1.7915610074996948,
      "learning_rate": 0.00019420833333333334,
      "loss": 0.8535,
      "step": 140
    },
    {
      "epoch": 0.15616866215512754,
      "grad_norm": 2.5494329929351807,
      "learning_rate": 0.00019379166666666668,
      "loss": 0.7667,
      "step": 150
    },
    {
      "epoch": 0.1665799062988027,
      "grad_norm": 1.2717150449752808,
      "learning_rate": 0.00019337500000000002,
      "loss": 0.864,
      "step": 160
    },
    {
      "epoch": 0.17699115044247787,
      "grad_norm": 1.154847502708435,
      "learning_rate": 0.00019295833333333334,
      "loss": 0.8349,
      "step": 170
    },
    {
      "epoch": 0.18740239458615304,
      "grad_norm": 1.092302680015564,
      "learning_rate": 0.00019254166666666668,
      "loss": 0.7869,
      "step": 180
    },
    {
      "epoch": 0.1978136387298282,
      "grad_norm": 1.4023083448410034,
      "learning_rate": 0.000192125,
      "loss": 0.7275,
      "step": 190
    },
    {
      "epoch": 0.20822488287350338,
      "grad_norm": 1.125715970993042,
      "learning_rate": 0.00019170833333333334,
      "loss": 0.6802,
      "step": 200
    },
    {
      "epoch": 0.21863612701717855,
      "grad_norm": 1.1369094848632812,
      "learning_rate": 0.00019129166666666668,
      "loss": 0.6758,
      "step": 210
    },
    {
      "epoch": 0.22904737116085372,
      "grad_norm": 1.0941975116729736,
      "learning_rate": 0.000190875,
      "loss": 0.7296,
      "step": 220
    },
    {
      "epoch": 0.2394586153045289,
      "grad_norm": 1.2387603521347046,
      "learning_rate": 0.00019045833333333333,
      "loss": 0.6259,
      "step": 230
    },
    {
      "epoch": 0.24986985944820406,
      "grad_norm": 1.0422847270965576,
      "learning_rate": 0.00019004166666666667,
      "loss": 0.6556,
      "step": 240
    },
    {
      "epoch": 0.2602811035918792,
      "grad_norm": 1.0612941980361938,
      "learning_rate": 0.00018962500000000001,
      "loss": 0.7754,
      "step": 250
    },
    {
      "epoch": 0.2706923477355544,
      "grad_norm": 1.156324028968811,
      "learning_rate": 0.00018920833333333336,
      "loss": 0.6635,
      "step": 260
    },
    {
      "epoch": 0.28110359187922956,
      "grad_norm": 1.1981353759765625,
      "learning_rate": 0.00018879166666666667,
      "loss": 0.6775,
      "step": 270
    },
    {
      "epoch": 0.29151483602290473,
      "grad_norm": 0.965570330619812,
      "learning_rate": 0.000188375,
      "loss": 0.6716,
      "step": 280
    },
    {
      "epoch": 0.3019260801665799,
      "grad_norm": 1.2502814531326294,
      "learning_rate": 0.00018795833333333335,
      "loss": 0.7385,
      "step": 290
    },
    {
      "epoch": 0.31233732431025507,
      "grad_norm": 1.271468162536621,
      "learning_rate": 0.0001875416666666667,
      "loss": 0.6642,
      "step": 300
    },
    {
      "epoch": 0.32274856845393024,
      "grad_norm": 1.1858775615692139,
      "learning_rate": 0.000187125,
      "loss": 0.6841,
      "step": 310
    },
    {
      "epoch": 0.3331598125976054,
      "grad_norm": 0.9937679171562195,
      "learning_rate": 0.00018670833333333335,
      "loss": 0.724,
      "step": 320
    },
    {
      "epoch": 0.3435710567412806,
      "grad_norm": 0.9969467520713806,
      "learning_rate": 0.0001862916666666667,
      "loss": 0.6997,
      "step": 330
    },
    {
      "epoch": 0.35398230088495575,
      "grad_norm": 1.0661547183990479,
      "learning_rate": 0.000185875,
      "loss": 0.7308,
      "step": 340
    },
    {
      "epoch": 0.3643935450286309,
      "grad_norm": 1.2041041851043701,
      "learning_rate": 0.00018545833333333335,
      "loss": 0.5723,
      "step": 350
    },
    {
      "epoch": 0.3748047891723061,
      "grad_norm": 0.9985057711601257,
      "learning_rate": 0.00018504166666666666,
      "loss": 0.6555,
      "step": 360
    },
    {
      "epoch": 0.38521603331598125,
      "grad_norm": 0.8760703802108765,
      "learning_rate": 0.000184625,
      "loss": 0.7199,
      "step": 370
    },
    {
      "epoch": 0.3956272774596564,
      "grad_norm": 1.635406494140625,
      "learning_rate": 0.00018420833333333334,
      "loss": 0.7235,
      "step": 380
    },
    {
      "epoch": 0.4060385216033316,
      "grad_norm": 1.1121442317962646,
      "learning_rate": 0.00018379166666666668,
      "loss": 0.6547,
      "step": 390
    },
    {
      "epoch": 0.41644976574700676,
      "grad_norm": 1.0640859603881836,
      "learning_rate": 0.000183375,
      "loss": 0.6592,
      "step": 400
    },
    {
      "epoch": 0.42686100989068193,
      "grad_norm": 0.9966997504234314,
      "learning_rate": 0.00018295833333333334,
      "loss": 0.6257,
      "step": 410
    },
    {
      "epoch": 0.4372722540343571,
      "grad_norm": 1.0152125358581543,
      "learning_rate": 0.00018254166666666668,
      "loss": 0.6085,
      "step": 420
    },
    {
      "epoch": 0.44768349817803227,
      "grad_norm": 1.0823588371276855,
      "learning_rate": 0.00018212500000000002,
      "loss": 0.7558,
      "step": 430
    },
    {
      "epoch": 0.45809474232170744,
      "grad_norm": 1.3496315479278564,
      "learning_rate": 0.00018170833333333334,
      "loss": 0.7546,
      "step": 440
    },
    {
      "epoch": 0.4685059864653826,
      "grad_norm": 1.0493782758712769,
      "learning_rate": 0.00018129166666666668,
      "loss": 0.6263,
      "step": 450
    },
    {
      "epoch": 0.4789172306090578,
      "grad_norm": 1.5099130868911743,
      "learning_rate": 0.00018087500000000002,
      "loss": 0.7067,
      "step": 460
    },
    {
      "epoch": 0.48932847475273294,
      "grad_norm": 0.9170027375221252,
      "learning_rate": 0.00018045833333333336,
      "loss": 0.6472,
      "step": 470
    },
    {
      "epoch": 0.4997397188964081,
      "grad_norm": 1.1029690504074097,
      "learning_rate": 0.00018004166666666667,
      "loss": 0.6405,
      "step": 480
    },
    {
      "epoch": 0.5101509630400833,
      "grad_norm": 0.9742501974105835,
      "learning_rate": 0.000179625,
      "loss": 0.7054,
      "step": 490
    },
    {
      "epoch": 0.5205622071837585,
      "grad_norm": 0.9267579913139343,
      "learning_rate": 0.00017920833333333333,
      "loss": 0.6743,
      "step": 500
    },
    {
      "epoch": 0.5309734513274337,
      "grad_norm": 0.8946873545646667,
      "learning_rate": 0.00017879166666666667,
      "loss": 0.5379,
      "step": 510
    },
    {
      "epoch": 0.5413846954711088,
      "grad_norm": 0.8533874750137329,
      "learning_rate": 0.000178375,
      "loss": 0.6887,
      "step": 520
    },
    {
      "epoch": 0.551795939614784,
      "grad_norm": 0.7138382196426392,
      "learning_rate": 0.00017795833333333333,
      "loss": 0.6219,
      "step": 530
    },
    {
      "epoch": 0.5622071837584591,
      "grad_norm": 1.201784610748291,
      "learning_rate": 0.00017754166666666667,
      "loss": 0.6374,
      "step": 540
    },
    {
      "epoch": 0.5726184279021344,
      "grad_norm": 0.9919139742851257,
      "learning_rate": 0.000177125,
      "loss": 0.6247,
      "step": 550
    },
    {
      "epoch": 0.5830296720458095,
      "grad_norm": 0.752824068069458,
      "learning_rate": 0.00017670833333333335,
      "loss": 0.5994,
      "step": 560
    },
    {
      "epoch": 0.5934409161894847,
      "grad_norm": 1.215842843055725,
      "learning_rate": 0.00017629166666666666,
      "loss": 0.6255,
      "step": 570
    },
    {
      "epoch": 0.6038521603331598,
      "grad_norm": 0.8952391147613525,
      "learning_rate": 0.000175875,
      "loss": 0.6619,
      "step": 580
    },
    {
      "epoch": 0.614263404476835,
      "grad_norm": 1.1376656293869019,
      "learning_rate": 0.00017545833333333335,
      "loss": 0.5473,
      "step": 590
    },
    {
      "epoch": 0.6246746486205101,
      "grad_norm": 1.2701843976974487,
      "learning_rate": 0.0001750416666666667,
      "loss": 0.6934,
      "step": 600
    },
    {
      "epoch": 0.6350858927641854,
      "grad_norm": 1.6852750778198242,
      "learning_rate": 0.00017462500000000003,
      "loss": 0.6269,
      "step": 610
    },
    {
      "epoch": 0.6454971369078605,
      "grad_norm": 1.0766006708145142,
      "learning_rate": 0.00017420833333333334,
      "loss": 0.7728,
      "step": 620
    },
    {
      "epoch": 0.6559083810515357,
      "grad_norm": 1.1012284755706787,
      "learning_rate": 0.00017379166666666669,
      "loss": 0.5666,
      "step": 630
    },
    {
      "epoch": 0.6663196251952108,
      "grad_norm": 1.1767797470092773,
      "learning_rate": 0.000173375,
      "loss": 0.655,
      "step": 640
    },
    {
      "epoch": 0.676730869338886,
      "grad_norm": 1.398612141609192,
      "learning_rate": 0.00017295833333333334,
      "loss": 0.6149,
      "step": 650
    },
    {
      "epoch": 0.6871421134825612,
      "grad_norm": 1.019231915473938,
      "learning_rate": 0.00017254166666666665,
      "loss": 0.802,
      "step": 660
    },
    {
      "epoch": 0.6975533576262364,
      "grad_norm": 0.8238323926925659,
      "learning_rate": 0.000172125,
      "loss": 0.7186,
      "step": 670
    },
    {
      "epoch": 0.7079646017699115,
      "grad_norm": 0.9136590361595154,
      "learning_rate": 0.00017170833333333334,
      "loss": 0.6096,
      "step": 680
    },
    {
      "epoch": 0.7183758459135867,
      "grad_norm": 0.8354536890983582,
      "learning_rate": 0.00017129166666666668,
      "loss": 0.6776,
      "step": 690
    },
    {
      "epoch": 0.7287870900572618,
      "grad_norm": 0.9596216678619385,
      "learning_rate": 0.00017087500000000002,
      "loss": 0.5823,
      "step": 700
    },
    {
      "epoch": 0.7391983342009371,
      "grad_norm": 0.8857652544975281,
      "learning_rate": 0.00017045833333333333,
      "loss": 0.6403,
      "step": 710
    },
    {
      "epoch": 0.7496095783446122,
      "grad_norm": 0.9889802932739258,
      "learning_rate": 0.00017004166666666668,
      "loss": 0.6129,
      "step": 720
    },
    {
      "epoch": 0.7600208224882874,
      "grad_norm": 0.9221607446670532,
      "learning_rate": 0.00016962500000000002,
      "loss": 0.6736,
      "step": 730
    },
    {
      "epoch": 0.7704320666319625,
      "grad_norm": 0.7700627446174622,
      "learning_rate": 0.00016920833333333336,
      "loss": 0.6794,
      "step": 740
    },
    {
      "epoch": 0.7808433107756377,
      "grad_norm": 0.8640146851539612,
      "learning_rate": 0.00016879166666666667,
      "loss": 0.6896,
      "step": 750
    },
    {
      "epoch": 0.7912545549193128,
      "grad_norm": 0.9410618543624878,
      "learning_rate": 0.000168375,
      "loss": 0.6312,
      "step": 760
    },
    {
      "epoch": 0.8016657990629881,
      "grad_norm": 0.8948925137519836,
      "learning_rate": 0.00016795833333333335,
      "loss": 0.6275,
      "step": 770
    },
    {
      "epoch": 0.8120770432066632,
      "grad_norm": 0.9141526222229004,
      "learning_rate": 0.0001675416666666667,
      "loss": 0.6515,
      "step": 780
    },
    {
      "epoch": 0.8224882873503384,
      "grad_norm": 1.0165648460388184,
      "learning_rate": 0.000167125,
      "loss": 0.6116,
      "step": 790
    },
    {
      "epoch": 0.8328995314940135,
      "grad_norm": 0.8915822505950928,
      "learning_rate": 0.00016670833333333332,
      "loss": 0.6657,
      "step": 800
    },
    {
      "epoch": 0.8433107756376887,
      "grad_norm": 0.9602516889572144,
      "learning_rate": 0.00016629166666666667,
      "loss": 0.6087,
      "step": 810
    },
    {
      "epoch": 0.8537220197813639,
      "grad_norm": 1.1646236181259155,
      "learning_rate": 0.000165875,
      "loss": 0.5722,
      "step": 820
    },
    {
      "epoch": 0.8641332639250391,
      "grad_norm": 1.3967232704162598,
      "learning_rate": 0.00016545833333333335,
      "loss": 0.6722,
      "step": 830
    },
    {
      "epoch": 0.8745445080687142,
      "grad_norm": 1.0260688066482544,
      "learning_rate": 0.00016504166666666666,
      "loss": 0.6485,
      "step": 840
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 1.022973656654358,
      "learning_rate": 0.000164625,
      "loss": 0.6418,
      "step": 850
    },
    {
      "epoch": 0.8953669963560645,
      "grad_norm": 0.9809411764144897,
      "learning_rate": 0.00016420833333333334,
      "loss": 0.6205,
      "step": 860
    },
    {
      "epoch": 0.9057782404997398,
      "grad_norm": 0.9711146950721741,
      "learning_rate": 0.00016379166666666669,
      "loss": 0.6362,
      "step": 870
    },
    {
      "epoch": 0.9161894846434149,
      "grad_norm": 0.9185469150543213,
      "learning_rate": 0.000163375,
      "loss": 0.6973,
      "step": 880
    },
    {
      "epoch": 0.9266007287870901,
      "grad_norm": 0.9585161805152893,
      "learning_rate": 0.00016295833333333334,
      "loss": 0.6038,
      "step": 890
    },
    {
      "epoch": 0.9370119729307652,
      "grad_norm": 0.9867895245552063,
      "learning_rate": 0.00016254166666666668,
      "loss": 0.8094,
      "step": 900
    },
    {
      "epoch": 0.9474232170744404,
      "grad_norm": 0.973476231098175,
      "learning_rate": 0.00016212500000000002,
      "loss": 0.5838,
      "step": 910
    },
    {
      "epoch": 0.9578344612181156,
      "grad_norm": 1.291447639465332,
      "learning_rate": 0.00016170833333333334,
      "loss": 0.5434,
      "step": 920
    },
    {
      "epoch": 0.9682457053617908,
      "grad_norm": 0.8822397589683533,
      "learning_rate": 0.00016129166666666668,
      "loss": 0.5279,
      "step": 930
    },
    {
      "epoch": 0.9786569495054659,
      "grad_norm": 0.9348440170288086,
      "learning_rate": 0.000160875,
      "loss": 0.6217,
      "step": 940
    },
    {
      "epoch": 0.9890681936491411,
      "grad_norm": 1.3278865814208984,
      "learning_rate": 0.00016045833333333333,
      "loss": 0.6191,
      "step": 950
    },
    {
      "epoch": 0.9994794377928162,
      "grad_norm": 0.7858816385269165,
      "learning_rate": 0.00016004166666666668,
      "loss": 0.7205,
      "step": 960
    },
    {
      "epoch": 1.0093701197293077,
      "grad_norm": 1.1216644048690796,
      "learning_rate": 0.000159625,
      "loss": 0.5316,
      "step": 970
    },
    {
      "epoch": 1.019781363872983,
      "grad_norm": 1.0144352912902832,
      "learning_rate": 0.00015920833333333333,
      "loss": 0.5883,
      "step": 980
    },
    {
      "epoch": 1.030192608016658,
      "grad_norm": 1.0374215841293335,
      "learning_rate": 0.00015879166666666667,
      "loss": 0.5447,
      "step": 990
    },
    {
      "epoch": 1.0406038521603331,
      "grad_norm": 0.9841583967208862,
      "learning_rate": 0.00015837500000000001,
      "loss": 0.5506,
      "step": 1000
    },
    {
      "epoch": 1.0510150963040084,
      "grad_norm": 0.9373903870582581,
      "learning_rate": 0.00015795833333333333,
      "loss": 0.4999,
      "step": 1010
    },
    {
      "epoch": 1.0614263404476836,
      "grad_norm": 1.1616346836090088,
      "learning_rate": 0.00015754166666666667,
      "loss": 0.59,
      "step": 1020
    },
    {
      "epoch": 1.0718375845913586,
      "grad_norm": 1.3832422494888306,
      "learning_rate": 0.000157125,
      "loss": 0.5518,
      "step": 1030
    },
    {
      "epoch": 1.0822488287350338,
      "grad_norm": 1.1353687047958374,
      "learning_rate": 0.00015670833333333335,
      "loss": 0.6127,
      "step": 1040
    },
    {
      "epoch": 1.092660072878709,
      "grad_norm": 1.02752685546875,
      "learning_rate": 0.0001562916666666667,
      "loss": 0.5687,
      "step": 1050
    },
    {
      "epoch": 1.1030713170223843,
      "grad_norm": 0.9549785256385803,
      "learning_rate": 0.000155875,
      "loss": 0.5643,
      "step": 1060
    },
    {
      "epoch": 1.1134825611660593,
      "grad_norm": 1.0074914693832397,
      "learning_rate": 0.00015545833333333335,
      "loss": 0.5901,
      "step": 1070
    },
    {
      "epoch": 1.1238938053097345,
      "grad_norm": 1.1108520030975342,
      "learning_rate": 0.0001550416666666667,
      "loss": 0.6008,
      "step": 1080
    },
    {
      "epoch": 1.1343050494534097,
      "grad_norm": 0.9748697280883789,
      "learning_rate": 0.000154625,
      "loss": 0.6178,
      "step": 1090
    },
    {
      "epoch": 1.144716293597085,
      "grad_norm": 0.9099568128585815,
      "learning_rate": 0.00015420833333333335,
      "loss": 0.4706,
      "step": 1100
    },
    {
      "epoch": 1.15512753774076,
      "grad_norm": 1.0737518072128296,
      "learning_rate": 0.00015379166666666666,
      "loss": 0.555,
      "step": 1110
    },
    {
      "epoch": 1.1655387818844352,
      "grad_norm": 1.0149807929992676,
      "learning_rate": 0.000153375,
      "loss": 0.546,
      "step": 1120
    },
    {
      "epoch": 1.1759500260281104,
      "grad_norm": 0.9471774101257324,
      "learning_rate": 0.00015295833333333334,
      "loss": 0.4342,
      "step": 1130
    },
    {
      "epoch": 1.1863612701717856,
      "grad_norm": 0.8046207427978516,
      "learning_rate": 0.00015254166666666668,
      "loss": 0.4306,
      "step": 1140
    },
    {
      "epoch": 1.1967725143154606,
      "grad_norm": 0.9498929977416992,
      "learning_rate": 0.000152125,
      "loss": 0.4693,
      "step": 1150
    },
    {
      "epoch": 1.2071837584591358,
      "grad_norm": 0.9749213457107544,
      "learning_rate": 0.00015170833333333334,
      "loss": 0.5002,
      "step": 1160
    },
    {
      "epoch": 1.217595002602811,
      "grad_norm": 0.9561901688575745,
      "learning_rate": 0.00015129166666666668,
      "loss": 0.5235,
      "step": 1170
    },
    {
      "epoch": 1.2280062467464863,
      "grad_norm": 1.008613109588623,
      "learning_rate": 0.00015087500000000002,
      "loss": 0.5654,
      "step": 1180
    },
    {
      "epoch": 1.2384174908901613,
      "grad_norm": 0.9319539666175842,
      "learning_rate": 0.00015045833333333334,
      "loss": 0.5017,
      "step": 1190
    },
    {
      "epoch": 1.2488287350338365,
      "grad_norm": 1.1879366636276245,
      "learning_rate": 0.00015004166666666668,
      "loss": 0.5375,
      "step": 1200
    },
    {
      "epoch": 1.2592399791775117,
      "grad_norm": 0.8424298167228699,
      "learning_rate": 0.00014962500000000002,
      "loss": 0.5311,
      "step": 1210
    },
    {
      "epoch": 1.269651223321187,
      "grad_norm": 1.096511960029602,
      "learning_rate": 0.00014920833333333336,
      "loss": 0.5729,
      "step": 1220
    },
    {
      "epoch": 1.280062467464862,
      "grad_norm": 1.0717260837554932,
      "learning_rate": 0.00014879166666666667,
      "loss": 0.5453,
      "step": 1230
    },
    {
      "epoch": 1.2904737116085372,
      "grad_norm": 1.2395129203796387,
      "learning_rate": 0.000148375,
      "loss": 0.6039,
      "step": 1240
    },
    {
      "epoch": 1.3008849557522124,
      "grad_norm": 1.2323378324508667,
      "learning_rate": 0.00014795833333333333,
      "loss": 0.5481,
      "step": 1250
    },
    {
      "epoch": 1.3112961998958876,
      "grad_norm": 1.2505011558532715,
      "learning_rate": 0.00014754166666666667,
      "loss": 0.5573,
      "step": 1260
    },
    {
      "epoch": 1.3217074440395629,
      "grad_norm": 0.9349163770675659,
      "learning_rate": 0.000147125,
      "loss": 0.5359,
      "step": 1270
    },
    {
      "epoch": 1.3321186881832379,
      "grad_norm": 1.0740805864334106,
      "learning_rate": 0.00014670833333333333,
      "loss": 0.4897,
      "step": 1280
    },
    {
      "epoch": 1.342529932326913,
      "grad_norm": 0.881152331829071,
      "learning_rate": 0.00014629166666666667,
      "loss": 0.4959,
      "step": 1290
    },
    {
      "epoch": 1.3529411764705883,
      "grad_norm": 1.0848314762115479,
      "learning_rate": 0.000145875,
      "loss": 0.618,
      "step": 1300
    },
    {
      "epoch": 1.3633524206142633,
      "grad_norm": 1.1704062223434448,
      "learning_rate": 0.00014545833333333335,
      "loss": 0.468,
      "step": 1310
    },
    {
      "epoch": 1.3737636647579385,
      "grad_norm": 0.8764092326164246,
      "learning_rate": 0.00014504166666666666,
      "loss": 0.5461,
      "step": 1320
    },
    {
      "epoch": 1.3841749089016138,
      "grad_norm": 0.8495427370071411,
      "learning_rate": 0.000144625,
      "loss": 0.5464,
      "step": 1330
    },
    {
      "epoch": 1.394586153045289,
      "grad_norm": 0.9854939579963684,
      "learning_rate": 0.00014420833333333335,
      "loss": 0.5045,
      "step": 1340
    },
    {
      "epoch": 1.4049973971889642,
      "grad_norm": 1.4169365167617798,
      "learning_rate": 0.0001437916666666667,
      "loss": 0.556,
      "step": 1350
    },
    {
      "epoch": 1.4154086413326392,
      "grad_norm": 1.6126189231872559,
      "learning_rate": 0.000143375,
      "loss": 0.5497,
      "step": 1360
    },
    {
      "epoch": 1.4258198854763144,
      "grad_norm": 0.9926950335502625,
      "learning_rate": 0.00014295833333333334,
      "loss": 0.5612,
      "step": 1370
    },
    {
      "epoch": 1.4362311296199897,
      "grad_norm": 1.1332557201385498,
      "learning_rate": 0.00014254166666666668,
      "loss": 0.6843,
      "step": 1380
    },
    {
      "epoch": 1.4466423737636647,
      "grad_norm": 0.8238005638122559,
      "learning_rate": 0.000142125,
      "loss": 0.4628,
      "step": 1390
    },
    {
      "epoch": 1.45705361790734,
      "grad_norm": 1.1631673574447632,
      "learning_rate": 0.00014170833333333334,
      "loss": 0.5151,
      "step": 1400
    },
    {
      "epoch": 1.4674648620510151,
      "grad_norm": 0.9485073089599609,
      "learning_rate": 0.00014129166666666665,
      "loss": 0.5387,
      "step": 1410
    },
    {
      "epoch": 1.4778761061946903,
      "grad_norm": 0.7761648297309875,
      "learning_rate": 0.000140875,
      "loss": 0.6044,
      "step": 1420
    },
    {
      "epoch": 1.4882873503383656,
      "grad_norm": 1.1801384687423706,
      "learning_rate": 0.00014045833333333334,
      "loss": 0.5268,
      "step": 1430
    },
    {
      "epoch": 1.4986985944820406,
      "grad_norm": 1.186410665512085,
      "learning_rate": 0.00014004166666666668,
      "loss": 0.5664,
      "step": 1440
    },
    {
      "epoch": 1.5091098386257158,
      "grad_norm": 1.1231354475021362,
      "learning_rate": 0.00013962500000000002,
      "loss": 0.5434,
      "step": 1450
    },
    {
      "epoch": 1.519521082769391,
      "grad_norm": 1.0179232358932495,
      "learning_rate": 0.00013920833333333333,
      "loss": 0.5483,
      "step": 1460
    },
    {
      "epoch": 1.529932326913066,
      "grad_norm": 1.1881080865859985,
      "learning_rate": 0.00013879166666666667,
      "loss": 0.5059,
      "step": 1470
    },
    {
      "epoch": 1.5403435710567412,
      "grad_norm": 0.9521448612213135,
      "learning_rate": 0.00013837500000000002,
      "loss": 0.4966,
      "step": 1480
    },
    {
      "epoch": 1.5507548152004165,
      "grad_norm": 1.0438371896743774,
      "learning_rate": 0.00013795833333333336,
      "loss": 0.5274,
      "step": 1490
    },
    {
      "epoch": 1.5611660593440915,
      "grad_norm": 1.2904707193374634,
      "learning_rate": 0.00013754166666666667,
      "loss": 0.4859,
      "step": 1500
    },
    {
      "epoch": 1.571577303487767,
      "grad_norm": 1.03959059715271,
      "learning_rate": 0.000137125,
      "loss": 0.535,
      "step": 1510
    },
    {
      "epoch": 1.581988547631442,
      "grad_norm": 1.07300865650177,
      "learning_rate": 0.00013670833333333335,
      "loss": 0.6205,
      "step": 1520
    },
    {
      "epoch": 1.5923997917751171,
      "grad_norm": 0.9650189280509949,
      "learning_rate": 0.0001362916666666667,
      "loss": 0.5431,
      "step": 1530
    },
    {
      "epoch": 1.6028110359187924,
      "grad_norm": 1.3832738399505615,
      "learning_rate": 0.000135875,
      "loss": 0.4966,
      "step": 1540
    },
    {
      "epoch": 1.6132222800624674,
      "grad_norm": 1.2777434587478638,
      "learning_rate": 0.00013545833333333332,
      "loss": 0.4938,
      "step": 1550
    },
    {
      "epoch": 1.6236335242061426,
      "grad_norm": 1.043261170387268,
      "learning_rate": 0.00013504166666666666,
      "loss": 0.5203,
      "step": 1560
    },
    {
      "epoch": 1.6340447683498178,
      "grad_norm": 1.0428402423858643,
      "learning_rate": 0.000134625,
      "loss": 0.545,
      "step": 1570
    },
    {
      "epoch": 1.6444560124934928,
      "grad_norm": 1.4831321239471436,
      "learning_rate": 0.00013420833333333335,
      "loss": 0.5342,
      "step": 1580
    },
    {
      "epoch": 1.6548672566371683,
      "grad_norm": 1.0979480743408203,
      "learning_rate": 0.00013379166666666666,
      "loss": 0.6162,
      "step": 1590
    },
    {
      "epoch": 1.6652785007808433,
      "grad_norm": 1.1031631231307983,
      "learning_rate": 0.000133375,
      "loss": 0.5204,
      "step": 1600
    },
    {
      "epoch": 1.6756897449245185,
      "grad_norm": 1.0679569244384766,
      "learning_rate": 0.00013295833333333334,
      "loss": 0.5548,
      "step": 1610
    },
    {
      "epoch": 1.6861009890681937,
      "grad_norm": 0.9606364965438843,
      "learning_rate": 0.00013254166666666669,
      "loss": 0.5999,
      "step": 1620
    },
    {
      "epoch": 1.6965122332118687,
      "grad_norm": 1.1904724836349487,
      "learning_rate": 0.000132125,
      "loss": 0.5386,
      "step": 1630
    },
    {
      "epoch": 1.706923477355544,
      "grad_norm": 1.049167513847351,
      "learning_rate": 0.00013170833333333334,
      "loss": 0.6181,
      "step": 1640
    },
    {
      "epoch": 1.7173347214992192,
      "grad_norm": 0.9151588678359985,
      "learning_rate": 0.00013129166666666668,
      "loss": 0.4953,
      "step": 1650
    },
    {
      "epoch": 1.7277459656428942,
      "grad_norm": 1.008571982383728,
      "learning_rate": 0.00013087500000000002,
      "loss": 0.4829,
      "step": 1660
    },
    {
      "epoch": 1.7381572097865696,
      "grad_norm": 1.130350947380066,
      "learning_rate": 0.00013045833333333334,
      "loss": 0.5664,
      "step": 1670
    },
    {
      "epoch": 1.7485684539302446,
      "grad_norm": 0.8882501125335693,
      "learning_rate": 0.00013004166666666668,
      "loss": 0.5407,
      "step": 1680
    },
    {
      "epoch": 1.7589796980739199,
      "grad_norm": 1.2144227027893066,
      "learning_rate": 0.000129625,
      "loss": 0.5879,
      "step": 1690
    },
    {
      "epoch": 1.769390942217595,
      "grad_norm": 1.1437244415283203,
      "learning_rate": 0.00012920833333333333,
      "loss": 0.5108,
      "step": 1700
    },
    {
      "epoch": 1.77980218636127,
      "grad_norm": 1.2752816677093506,
      "learning_rate": 0.00012879166666666668,
      "loss": 0.5321,
      "step": 1710
    },
    {
      "epoch": 1.7902134305049453,
      "grad_norm": 1.086206316947937,
      "learning_rate": 0.000128375,
      "loss": 0.5109,
      "step": 1720
    },
    {
      "epoch": 1.8006246746486205,
      "grad_norm": 0.846265435218811,
      "learning_rate": 0.00012795833333333333,
      "loss": 0.5249,
      "step": 1730
    },
    {
      "epoch": 1.8110359187922955,
      "grad_norm": 0.9690593481063843,
      "learning_rate": 0.00012754166666666667,
      "loss": 0.5117,
      "step": 1740
    },
    {
      "epoch": 1.821447162935971,
      "grad_norm": 0.8050377368927002,
      "learning_rate": 0.00012712500000000001,
      "loss": 0.561,
      "step": 1750
    },
    {
      "epoch": 1.831858407079646,
      "grad_norm": 0.8127462863922119,
      "learning_rate": 0.00012670833333333333,
      "loss": 0.6004,
      "step": 1760
    },
    {
      "epoch": 1.8422696512233212,
      "grad_norm": 0.9077056050300598,
      "learning_rate": 0.00012629166666666667,
      "loss": 0.5841,
      "step": 1770
    },
    {
      "epoch": 1.8526808953669964,
      "grad_norm": 1.116462230682373,
      "learning_rate": 0.000125875,
      "loss": 0.5658,
      "step": 1780
    },
    {
      "epoch": 1.8630921395106714,
      "grad_norm": 0.7550406455993652,
      "learning_rate": 0.00012545833333333335,
      "loss": 0.6165,
      "step": 1790
    },
    {
      "epoch": 1.8735033836543467,
      "grad_norm": 1.0435996055603027,
      "learning_rate": 0.00012504166666666667,
      "loss": 0.5779,
      "step": 1800
    },
    {
      "epoch": 1.8839146277980219,
      "grad_norm": 1.260796308517456,
      "learning_rate": 0.000124625,
      "loss": 0.4803,
      "step": 1810
    },
    {
      "epoch": 1.8943258719416969,
      "grad_norm": 0.8643637895584106,
      "learning_rate": 0.00012420833333333335,
      "loss": 0.5749,
      "step": 1820
    },
    {
      "epoch": 1.9047371160853723,
      "grad_norm": 0.910472571849823,
      "learning_rate": 0.0001237916666666667,
      "loss": 0.5739,
      "step": 1830
    },
    {
      "epoch": 1.9151483602290473,
      "grad_norm": 1.1470736265182495,
      "learning_rate": 0.000123375,
      "loss": 0.5848,
      "step": 1840
    },
    {
      "epoch": 1.9255596043727226,
      "grad_norm": 1.0993155241012573,
      "learning_rate": 0.00012295833333333332,
      "loss": 0.5282,
      "step": 1850
    },
    {
      "epoch": 1.9359708485163978,
      "grad_norm": 1.1167094707489014,
      "learning_rate": 0.00012254166666666666,
      "loss": 0.5421,
      "step": 1860
    },
    {
      "epoch": 1.9463820926600728,
      "grad_norm": 0.9785841703414917,
      "learning_rate": 0.000122125,
      "loss": 0.5054,
      "step": 1870
    },
    {
      "epoch": 1.956793336803748,
      "grad_norm": 0.7939749956130981,
      "learning_rate": 0.00012170833333333334,
      "loss": 0.5482,
      "step": 1880
    },
    {
      "epoch": 1.9672045809474232,
      "grad_norm": 1.199192762374878,
      "learning_rate": 0.00012129166666666667,
      "loss": 0.4975,
      "step": 1890
    },
    {
      "epoch": 1.9776158250910982,
      "grad_norm": 1.405483603477478,
      "learning_rate": 0.00012087500000000001,
      "loss": 0.6522,
      "step": 1900
    },
    {
      "epoch": 1.9880270692347737,
      "grad_norm": 1.3604921102523804,
      "learning_rate": 0.00012045833333333334,
      "loss": 0.5503,
      "step": 1910
    },
    {
      "epoch": 1.9984383133784487,
      "grad_norm": 0.9543437957763672,
      "learning_rate": 0.00012004166666666668,
      "loss": 0.5551,
      "step": 1920
    },
    {
      "epoch": 2.0083289953149404,
      "grad_norm": 0.9927816390991211,
      "learning_rate": 0.00011962500000000001,
      "loss": 0.4222,
      "step": 1930
    },
    {
      "epoch": 2.0187402394586154,
      "grad_norm": 0.7834363579750061,
      "learning_rate": 0.00011920833333333335,
      "loss": 0.4492,
      "step": 1940
    },
    {
      "epoch": 2.0291514836022904,
      "grad_norm": 0.9656623005867004,
      "learning_rate": 0.00011879166666666668,
      "loss": 0.4482,
      "step": 1950
    },
    {
      "epoch": 2.039562727745966,
      "grad_norm": 0.7927660942077637,
      "learning_rate": 0.00011837500000000002,
      "loss": 0.4113,
      "step": 1960
    },
    {
      "epoch": 2.049973971889641,
      "grad_norm": 1.0019478797912598,
      "learning_rate": 0.00011795833333333335,
      "loss": 0.4236,
      "step": 1970
    },
    {
      "epoch": 2.060385216033316,
      "grad_norm": 1.0382826328277588,
      "learning_rate": 0.00011754166666666669,
      "loss": 0.3928,
      "step": 1980
    },
    {
      "epoch": 2.0707964601769913,
      "grad_norm": 1.054792046546936,
      "learning_rate": 0.000117125,
      "loss": 0.3607,
      "step": 1990
    },
    {
      "epoch": 2.0812077043206663,
      "grad_norm": 1.2541656494140625,
      "learning_rate": 0.00011670833333333333,
      "loss": 0.5506,
      "step": 2000
    },
    {
      "epoch": 2.0916189484643413,
      "grad_norm": 1.8471565246582031,
      "learning_rate": 0.00011629166666666667,
      "loss": 0.4426,
      "step": 2010
    },
    {
      "epoch": 2.1020301926080167,
      "grad_norm": 1.1316235065460205,
      "learning_rate": 0.000115875,
      "loss": 0.4581,
      "step": 2020
    },
    {
      "epoch": 2.1124414367516917,
      "grad_norm": 1.1852366924285889,
      "learning_rate": 0.00011545833333333334,
      "loss": 0.3955,
      "step": 2030
    },
    {
      "epoch": 2.122852680895367,
      "grad_norm": 0.8964291214942932,
      "learning_rate": 0.00011504166666666667,
      "loss": 0.4292,
      "step": 2040
    },
    {
      "epoch": 2.133263925039042,
      "grad_norm": 0.9482913017272949,
      "learning_rate": 0.00011462500000000001,
      "loss": 0.4063,
      "step": 2050
    },
    {
      "epoch": 2.143675169182717,
      "grad_norm": 1.081104040145874,
      "learning_rate": 0.00011420833333333334,
      "loss": 0.4536,
      "step": 2060
    },
    {
      "epoch": 2.1540864133263926,
      "grad_norm": 1.3784219026565552,
      "learning_rate": 0.00011379166666666668,
      "loss": 0.5231,
      "step": 2070
    },
    {
      "epoch": 2.1644976574700676,
      "grad_norm": 1.2771320343017578,
      "learning_rate": 0.000113375,
      "loss": 0.4561,
      "step": 2080
    },
    {
      "epoch": 2.174908901613743,
      "grad_norm": 1.0900886058807373,
      "learning_rate": 0.00011295833333333335,
      "loss": 0.4516,
      "step": 2090
    },
    {
      "epoch": 2.185320145757418,
      "grad_norm": 1.0569919347763062,
      "learning_rate": 0.00011254166666666667,
      "loss": 0.3867,
      "step": 2100
    },
    {
      "epoch": 2.195731389901093,
      "grad_norm": 1.4584932327270508,
      "learning_rate": 0.00011212500000000001,
      "loss": 0.418,
      "step": 2110
    },
    {
      "epoch": 2.2061426340447685,
      "grad_norm": 0.6902841329574585,
      "learning_rate": 0.00011170833333333334,
      "loss": 0.4077,
      "step": 2120
    },
    {
      "epoch": 2.2165538781884435,
      "grad_norm": 1.2164291143417358,
      "learning_rate": 0.00011129166666666668,
      "loss": 0.5718,
      "step": 2130
    },
    {
      "epoch": 2.2269651223321185,
      "grad_norm": 1.058414101600647,
      "learning_rate": 0.000110875,
      "loss": 0.4368,
      "step": 2140
    },
    {
      "epoch": 2.237376366475794,
      "grad_norm": 1.180930733680725,
      "learning_rate": 0.00011045833333333333,
      "loss": 0.5069,
      "step": 2150
    },
    {
      "epoch": 2.247787610619469,
      "grad_norm": 2.283329486846924,
      "learning_rate": 0.00011004166666666667,
      "loss": 0.4466,
      "step": 2160
    },
    {
      "epoch": 2.258198854763144,
      "grad_norm": 1.1396514177322388,
      "learning_rate": 0.000109625,
      "loss": 0.4964,
      "step": 2170
    },
    {
      "epoch": 2.2686100989068194,
      "grad_norm": 1.4050918817520142,
      "learning_rate": 0.00010920833333333334,
      "loss": 0.4776,
      "step": 2180
    },
    {
      "epoch": 2.2790213430504944,
      "grad_norm": 1.1574746370315552,
      "learning_rate": 0.00010879166666666666,
      "loss": 0.4227,
      "step": 2190
    },
    {
      "epoch": 2.28943258719417,
      "grad_norm": 1.0701229572296143,
      "learning_rate": 0.000108375,
      "loss": 0.4743,
      "step": 2200
    },
    {
      "epoch": 2.299843831337845,
      "grad_norm": 1.122308373451233,
      "learning_rate": 0.00010795833333333333,
      "loss": 0.5185,
      "step": 2210
    },
    {
      "epoch": 2.31025507548152,
      "grad_norm": 0.9449405670166016,
      "learning_rate": 0.00010754166666666667,
      "loss": 0.4194,
      "step": 2220
    },
    {
      "epoch": 2.3206663196251953,
      "grad_norm": 1.2834762334823608,
      "learning_rate": 0.00010712500000000002,
      "loss": 0.4837,
      "step": 2230
    },
    {
      "epoch": 2.3310775637688703,
      "grad_norm": 1.1489782333374023,
      "learning_rate": 0.00010670833333333334,
      "loss": 0.4606,
      "step": 2240
    },
    {
      "epoch": 2.3414888079125458,
      "grad_norm": 1.0569758415222168,
      "learning_rate": 0.00010629166666666668,
      "loss": 0.5056,
      "step": 2250
    },
    {
      "epoch": 2.3519000520562208,
      "grad_norm": 1.2946444749832153,
      "learning_rate": 0.00010587500000000001,
      "loss": 0.4832,
      "step": 2260
    },
    {
      "epoch": 2.3623112961998958,
      "grad_norm": 1.032813549041748,
      "learning_rate": 0.00010545833333333335,
      "loss": 0.496,
      "step": 2270
    },
    {
      "epoch": 2.372722540343571,
      "grad_norm": 1.1081656217575073,
      "learning_rate": 0.00010504166666666668,
      "loss": 0.4423,
      "step": 2280
    },
    {
      "epoch": 2.383133784487246,
      "grad_norm": 1.2804787158966064,
      "learning_rate": 0.000104625,
      "loss": 0.4765,
      "step": 2290
    },
    {
      "epoch": 2.3935450286309212,
      "grad_norm": 1.2887171506881714,
      "learning_rate": 0.00010420833333333334,
      "loss": 0.4326,
      "step": 2300
    },
    {
      "epoch": 2.4039562727745967,
      "grad_norm": 1.2103713750839233,
      "learning_rate": 0.00010379166666666666,
      "loss": 0.4259,
      "step": 2310
    },
    {
      "epoch": 2.4143675169182717,
      "grad_norm": 0.9183862805366516,
      "learning_rate": 0.000103375,
      "loss": 0.3378,
      "step": 2320
    },
    {
      "epoch": 2.4247787610619467,
      "grad_norm": 1.483837366104126,
      "learning_rate": 0.00010295833333333333,
      "loss": 0.4311,
      "step": 2330
    },
    {
      "epoch": 2.435190005205622,
      "grad_norm": 1.1243866682052612,
      "learning_rate": 0.00010254166666666667,
      "loss": 0.4696,
      "step": 2340
    },
    {
      "epoch": 2.445601249349297,
      "grad_norm": 1.4769861698150635,
      "learning_rate": 0.000102125,
      "loss": 0.524,
      "step": 2350
    },
    {
      "epoch": 2.4560124934929726,
      "grad_norm": 1.2417573928833008,
      "learning_rate": 0.00010170833333333334,
      "loss": 0.4526,
      "step": 2360
    },
    {
      "epoch": 2.4664237376366476,
      "grad_norm": 1.2225010395050049,
      "learning_rate": 0.00010129166666666667,
      "loss": 0.4603,
      "step": 2370
    },
    {
      "epoch": 2.4768349817803226,
      "grad_norm": 1.2873847484588623,
      "learning_rate": 0.00010087500000000001,
      "loss": 0.4211,
      "step": 2380
    },
    {
      "epoch": 2.487246225923998,
      "grad_norm": 1.2819379568099976,
      "learning_rate": 0.00010045833333333334,
      "loss": 0.4679,
      "step": 2390
    },
    {
      "epoch": 2.497657470067673,
      "grad_norm": 1.0822672843933105,
      "learning_rate": 0.00010004166666666668,
      "loss": 0.4568,
      "step": 2400
    },
    {
      "epoch": 2.5080687142113485,
      "grad_norm": 1.0502815246582031,
      "learning_rate": 9.9625e-05,
      "loss": 0.4284,
      "step": 2410
    },
    {
      "epoch": 2.5184799583550235,
      "grad_norm": 2.036215305328369,
      "learning_rate": 9.920833333333334e-05,
      "loss": 0.5077,
      "step": 2420
    },
    {
      "epoch": 2.5288912024986985,
      "grad_norm": 1.3241667747497559,
      "learning_rate": 9.879166666666666e-05,
      "loss": 0.4237,
      "step": 2430
    },
    {
      "epoch": 2.539302446642374,
      "grad_norm": 1.3151954412460327,
      "learning_rate": 9.8375e-05,
      "loss": 0.4616,
      "step": 2440
    },
    {
      "epoch": 2.549713690786049,
      "grad_norm": 1.198267936706543,
      "learning_rate": 9.795833333333335e-05,
      "loss": 0.4208,
      "step": 2450
    },
    {
      "epoch": 2.560124934929724,
      "grad_norm": 1.060966968536377,
      "learning_rate": 9.754166666666667e-05,
      "loss": 0.4901,
      "step": 2460
    },
    {
      "epoch": 2.5705361790733994,
      "grad_norm": 1.096092939376831,
      "learning_rate": 9.7125e-05,
      "loss": 0.434,
      "step": 2470
    },
    {
      "epoch": 2.5809474232170744,
      "grad_norm": 1.4607443809509277,
      "learning_rate": 9.670833333333333e-05,
      "loss": 0.5339,
      "step": 2480
    },
    {
      "epoch": 2.5913586673607494,
      "grad_norm": 1.3779650926589966,
      "learning_rate": 9.629166666666667e-05,
      "loss": 0.4627,
      "step": 2490
    },
    {
      "epoch": 2.601769911504425,
      "grad_norm": 0.9531710147857666,
      "learning_rate": 9.5875e-05,
      "loss": 0.4185,
      "step": 2500
    },
    {
      "epoch": 2.6121811556481,
      "grad_norm": 1.3555033206939697,
      "learning_rate": 9.545833333333334e-05,
      "loss": 0.4297,
      "step": 2510
    },
    {
      "epoch": 2.6225923997917753,
      "grad_norm": 1.3027822971343994,
      "learning_rate": 9.504166666666667e-05,
      "loss": 0.4743,
      "step": 2520
    },
    {
      "epoch": 2.6330036439354503,
      "grad_norm": 1.2690298557281494,
      "learning_rate": 9.462500000000001e-05,
      "loss": 0.4455,
      "step": 2530
    },
    {
      "epoch": 2.6434148880791257,
      "grad_norm": 1.0129756927490234,
      "learning_rate": 9.420833333333334e-05,
      "loss": 0.4145,
      "step": 2540
    },
    {
      "epoch": 2.6538261322228007,
      "grad_norm": 0.866169810295105,
      "learning_rate": 9.379166666666667e-05,
      "loss": 0.4622,
      "step": 2550
    },
    {
      "epoch": 2.6642373763664757,
      "grad_norm": 1.4092011451721191,
      "learning_rate": 9.3375e-05,
      "loss": 0.4696,
      "step": 2560
    },
    {
      "epoch": 2.674648620510151,
      "grad_norm": 0.9800272583961487,
      "learning_rate": 9.295833333333333e-05,
      "loss": 0.4325,
      "step": 2570
    },
    {
      "epoch": 2.685059864653826,
      "grad_norm": 1.1913902759552002,
      "learning_rate": 9.254166666666668e-05,
      "loss": 0.4569,
      "step": 2580
    },
    {
      "epoch": 2.695471108797501,
      "grad_norm": 1.429713487625122,
      "learning_rate": 9.2125e-05,
      "loss": 0.4674,
      "step": 2590
    },
    {
      "epoch": 2.7058823529411766,
      "grad_norm": 1.128502368927002,
      "learning_rate": 9.170833333333334e-05,
      "loss": 0.4483,
      "step": 2600
    },
    {
      "epoch": 2.7162935970848516,
      "grad_norm": 1.4540544748306274,
      "learning_rate": 9.129166666666667e-05,
      "loss": 0.4864,
      "step": 2610
    },
    {
      "epoch": 2.7267048412285266,
      "grad_norm": 1.161548137664795,
      "learning_rate": 9.0875e-05,
      "loss": 0.5706,
      "step": 2620
    },
    {
      "epoch": 2.737116085372202,
      "grad_norm": 1.6310784816741943,
      "learning_rate": 9.045833333333333e-05,
      "loss": 0.4039,
      "step": 2630
    },
    {
      "epoch": 2.747527329515877,
      "grad_norm": 1.1347562074661255,
      "learning_rate": 9.004166666666667e-05,
      "loss": 0.4469,
      "step": 2640
    },
    {
      "epoch": 2.757938573659552,
      "grad_norm": 1.1602872610092163,
      "learning_rate": 8.962500000000001e-05,
      "loss": 0.4426,
      "step": 2650
    },
    {
      "epoch": 2.7683498178032275,
      "grad_norm": 0.9956554174423218,
      "learning_rate": 8.920833333333334e-05,
      "loss": 0.4593,
      "step": 2660
    },
    {
      "epoch": 2.7787610619469025,
      "grad_norm": 1.623246669769287,
      "learning_rate": 8.879166666666668e-05,
      "loss": 0.4406,
      "step": 2670
    },
    {
      "epoch": 2.789172306090578,
      "grad_norm": 1.2868911027908325,
      "learning_rate": 8.837500000000001e-05,
      "loss": 0.4814,
      "step": 2680
    },
    {
      "epoch": 2.799583550234253,
      "grad_norm": 1.5386050939559937,
      "learning_rate": 8.795833333333335e-05,
      "loss": 0.4325,
      "step": 2690
    },
    {
      "epoch": 2.8099947943779284,
      "grad_norm": 0.925131618976593,
      "learning_rate": 8.754166666666666e-05,
      "loss": 0.4693,
      "step": 2700
    },
    {
      "epoch": 2.8204060385216034,
      "grad_norm": 0.9200794100761414,
      "learning_rate": 8.7125e-05,
      "loss": 0.4394,
      "step": 2710
    },
    {
      "epoch": 2.8308172826652784,
      "grad_norm": 1.2819395065307617,
      "learning_rate": 8.670833333333333e-05,
      "loss": 0.4496,
      "step": 2720
    },
    {
      "epoch": 2.841228526808954,
      "grad_norm": 1.2862424850463867,
      "learning_rate": 8.629166666666667e-05,
      "loss": 0.5177,
      "step": 2730
    },
    {
      "epoch": 2.851639770952629,
      "grad_norm": 1.2239999771118164,
      "learning_rate": 8.5875e-05,
      "loss": 0.4276,
      "step": 2740
    },
    {
      "epoch": 2.862051015096304,
      "grad_norm": 1.0430824756622314,
      "learning_rate": 8.545833333333334e-05,
      "loss": 0.4583,
      "step": 2750
    },
    {
      "epoch": 2.8724622592399793,
      "grad_norm": 1.3330588340759277,
      "learning_rate": 8.504166666666667e-05,
      "loss": 0.5151,
      "step": 2760
    },
    {
      "epoch": 2.8828735033836543,
      "grad_norm": 1.6461223363876343,
      "learning_rate": 8.4625e-05,
      "loss": 0.3935,
      "step": 2770
    },
    {
      "epoch": 2.8932847475273293,
      "grad_norm": 1.2316707372665405,
      "learning_rate": 8.420833333333334e-05,
      "loss": 0.48,
      "step": 2780
    },
    {
      "epoch": 2.903695991671005,
      "grad_norm": 1.1758652925491333,
      "learning_rate": 8.379166666666667e-05,
      "loss": 0.439,
      "step": 2790
    },
    {
      "epoch": 2.91410723581468,
      "grad_norm": 1.144391417503357,
      "learning_rate": 8.337500000000001e-05,
      "loss": 0.4605,
      "step": 2800
    },
    {
      "epoch": 2.924518479958355,
      "grad_norm": 1.1304064989089966,
      "learning_rate": 8.295833333333333e-05,
      "loss": 0.4258,
      "step": 2810
    },
    {
      "epoch": 2.9349297241020302,
      "grad_norm": 1.2203598022460938,
      "learning_rate": 8.254166666666668e-05,
      "loss": 0.4244,
      "step": 2820
    },
    {
      "epoch": 2.9453409682457052,
      "grad_norm": 1.1584584712982178,
      "learning_rate": 8.2125e-05,
      "loss": 0.4472,
      "step": 2830
    },
    {
      "epoch": 2.9557522123893807,
      "grad_norm": 1.1360650062561035,
      "learning_rate": 8.170833333333335e-05,
      "loss": 0.4241,
      "step": 2840
    },
    {
      "epoch": 2.9661634565330557,
      "grad_norm": 1.0261088609695435,
      "learning_rate": 8.129166666666666e-05,
      "loss": 0.4431,
      "step": 2850
    },
    {
      "epoch": 2.976574700676731,
      "grad_norm": 1.207010269165039,
      "learning_rate": 8.0875e-05,
      "loss": 0.4772,
      "step": 2860
    },
    {
      "epoch": 2.986985944820406,
      "grad_norm": 1.0351868867874146,
      "learning_rate": 8.045833333333334e-05,
      "loss": 0.4123,
      "step": 2870
    },
    {
      "epoch": 2.997397188964081,
      "grad_norm": 1.0329471826553345,
      "learning_rate": 8.004166666666667e-05,
      "loss": 0.3794,
      "step": 2880
    },
    {
      "epoch": 3.007287870900573,
      "grad_norm": 1.3351002931594849,
      "learning_rate": 7.962500000000001e-05,
      "loss": 0.4168,
      "step": 2890
    },
    {
      "epoch": 3.017699115044248,
      "grad_norm": 0.943784773349762,
      "learning_rate": 7.920833333333334e-05,
      "loss": 0.3074,
      "step": 2900
    },
    {
      "epoch": 3.028110359187923,
      "grad_norm": 1.0456507205963135,
      "learning_rate": 7.879166666666668e-05,
      "loss": 0.3583,
      "step": 2910
    },
    {
      "epoch": 3.0385216033315983,
      "grad_norm": 1.0007781982421875,
      "learning_rate": 7.8375e-05,
      "loss": 0.3808,
      "step": 2920
    },
    {
      "epoch": 3.0489328474752733,
      "grad_norm": 1.34638512134552,
      "learning_rate": 7.795833333333334e-05,
      "loss": 0.3424,
      "step": 2930
    },
    {
      "epoch": 3.0593440916189483,
      "grad_norm": 1.926309585571289,
      "learning_rate": 7.754166666666666e-05,
      "loss": 0.3845,
      "step": 2940
    },
    {
      "epoch": 3.0697553357626237,
      "grad_norm": 1.192808985710144,
      "learning_rate": 7.7125e-05,
      "loss": 0.3648,
      "step": 2950
    },
    {
      "epoch": 3.0801665799062987,
      "grad_norm": 1.2405411005020142,
      "learning_rate": 7.670833333333333e-05,
      "loss": 0.4619,
      "step": 2960
    },
    {
      "epoch": 3.090577824049974,
      "grad_norm": 1.0151933431625366,
      "learning_rate": 7.629166666666667e-05,
      "loss": 0.3584,
      "step": 2970
    },
    {
      "epoch": 3.100989068193649,
      "grad_norm": 1.7905350923538208,
      "learning_rate": 7.5875e-05,
      "loss": 0.3541,
      "step": 2980
    },
    {
      "epoch": 3.111400312337324,
      "grad_norm": 1.4807363748550415,
      "learning_rate": 7.545833333333334e-05,
      "loss": 0.4541,
      "step": 2990
    },
    {
      "epoch": 3.1218115564809996,
      "grad_norm": 1.2282313108444214,
      "learning_rate": 7.504166666666667e-05,
      "loss": 0.4009,
      "step": 3000
    },
    {
      "epoch": 3.1322228006246746,
      "grad_norm": 1.3905701637268066,
      "learning_rate": 7.4625e-05,
      "loss": 0.3518,
      "step": 3010
    },
    {
      "epoch": 3.1426340447683496,
      "grad_norm": 0.9421320557594299,
      "learning_rate": 7.420833333333334e-05,
      "loss": 0.3672,
      "step": 3020
    },
    {
      "epoch": 3.153045288912025,
      "grad_norm": 0.855960488319397,
      "learning_rate": 7.379166666666667e-05,
      "loss": 0.3692,
      "step": 3030
    },
    {
      "epoch": 3.1634565330557,
      "grad_norm": 1.1971511840820312,
      "learning_rate": 7.337500000000001e-05,
      "loss": 0.37,
      "step": 3040
    },
    {
      "epoch": 3.1738677771993755,
      "grad_norm": 1.2596534490585327,
      "learning_rate": 7.295833333333334e-05,
      "loss": 0.3755,
      "step": 3050
    },
    {
      "epoch": 3.1842790213430505,
      "grad_norm": 1.0966569185256958,
      "learning_rate": 7.254166666666668e-05,
      "loss": 0.3372,
      "step": 3060
    },
    {
      "epoch": 3.1946902654867255,
      "grad_norm": 1.5493748188018799,
      "learning_rate": 7.2125e-05,
      "loss": 0.3744,
      "step": 3070
    },
    {
      "epoch": 3.205101509630401,
      "grad_norm": 1.4007761478424072,
      "learning_rate": 7.170833333333333e-05,
      "loss": 0.3778,
      "step": 3080
    },
    {
      "epoch": 3.215512753774076,
      "grad_norm": 1.5889172554016113,
      "learning_rate": 7.129166666666667e-05,
      "loss": 0.4151,
      "step": 3090
    },
    {
      "epoch": 3.225923997917751,
      "grad_norm": 1.4899405241012573,
      "learning_rate": 7.0875e-05,
      "loss": 0.3787,
      "step": 3100
    },
    {
      "epoch": 3.2363352420614264,
      "grad_norm": 1.4690020084381104,
      "learning_rate": 7.045833333333334e-05,
      "loss": 0.3813,
      "step": 3110
    },
    {
      "epoch": 3.2467464862051014,
      "grad_norm": 1.196968674659729,
      "learning_rate": 7.004166666666667e-05,
      "loss": 0.3404,
      "step": 3120
    },
    {
      "epoch": 3.257157730348777,
      "grad_norm": 1.2663520574569702,
      "learning_rate": 6.962500000000001e-05,
      "loss": 0.4546,
      "step": 3130
    },
    {
      "epoch": 3.267568974492452,
      "grad_norm": 1.086880087852478,
      "learning_rate": 6.920833333333334e-05,
      "loss": 0.417,
      "step": 3140
    },
    {
      "epoch": 3.277980218636127,
      "grad_norm": 1.0637925863265991,
      "learning_rate": 6.879166666666667e-05,
      "loss": 0.3779,
      "step": 3150
    },
    {
      "epoch": 3.2883914627798023,
      "grad_norm": 1.0466058254241943,
      "learning_rate": 6.8375e-05,
      "loss": 0.4298,
      "step": 3160
    },
    {
      "epoch": 3.2988027069234773,
      "grad_norm": 1.2542874813079834,
      "learning_rate": 6.795833333333334e-05,
      "loss": 0.3546,
      "step": 3170
    },
    {
      "epoch": 3.3092139510671528,
      "grad_norm": 1.0692540407180786,
      "learning_rate": 6.754166666666666e-05,
      "loss": 0.38,
      "step": 3180
    },
    {
      "epoch": 3.3196251952108278,
      "grad_norm": 1.6422370672225952,
      "learning_rate": 6.7125e-05,
      "loss": 0.3546,
      "step": 3190
    },
    {
      "epoch": 3.3300364393545028,
      "grad_norm": 1.2388745546340942,
      "learning_rate": 6.670833333333333e-05,
      "loss": 0.3805,
      "step": 3200
    },
    {
      "epoch": 3.340447683498178,
      "grad_norm": 1.3252363204956055,
      "learning_rate": 6.629166666666667e-05,
      "loss": 0.3862,
      "step": 3210
    },
    {
      "epoch": 3.3508589276418532,
      "grad_norm": 1.3927541971206665,
      "learning_rate": 6.5875e-05,
      "loss": 0.4327,
      "step": 3220
    },
    {
      "epoch": 3.3612701717855282,
      "grad_norm": 1.1312706470489502,
      "learning_rate": 6.545833333333333e-05,
      "loss": 0.3934,
      "step": 3230
    },
    {
      "epoch": 3.3716814159292037,
      "grad_norm": 1.2615383863449097,
      "learning_rate": 6.504166666666667e-05,
      "loss": 0.38,
      "step": 3240
    },
    {
      "epoch": 3.3820926600728787,
      "grad_norm": 1.2607558965682983,
      "learning_rate": 6.4625e-05,
      "loss": 0.322,
      "step": 3250
    },
    {
      "epoch": 3.3925039042165537,
      "grad_norm": 0.9509683847427368,
      "learning_rate": 6.420833333333334e-05,
      "loss": 0.3482,
      "step": 3260
    },
    {
      "epoch": 3.402915148360229,
      "grad_norm": 1.4006731510162354,
      "learning_rate": 6.379166666666667e-05,
      "loss": 0.403,
      "step": 3270
    },
    {
      "epoch": 3.413326392503904,
      "grad_norm": 1.2457270622253418,
      "learning_rate": 6.337500000000001e-05,
      "loss": 0.3156,
      "step": 3280
    },
    {
      "epoch": 3.4237376366475796,
      "grad_norm": 1.1680009365081787,
      "learning_rate": 6.295833333333334e-05,
      "loss": 0.3879,
      "step": 3290
    },
    {
      "epoch": 3.4341488807912546,
      "grad_norm": 1.1525483131408691,
      "learning_rate": 6.254166666666666e-05,
      "loss": 0.4434,
      "step": 3300
    },
    {
      "epoch": 3.4445601249349296,
      "grad_norm": 1.0321340560913086,
      "learning_rate": 6.2125e-05,
      "loss": 0.3424,
      "step": 3310
    },
    {
      "epoch": 3.454971369078605,
      "grad_norm": 1.5190483331680298,
      "learning_rate": 6.170833333333333e-05,
      "loss": 0.4282,
      "step": 3320
    },
    {
      "epoch": 3.46538261322228,
      "grad_norm": 1.2648764848709106,
      "learning_rate": 6.129166666666667e-05,
      "loss": 0.3594,
      "step": 3330
    },
    {
      "epoch": 3.4757938573659555,
      "grad_norm": 1.3353387117385864,
      "learning_rate": 6.0875e-05,
      "loss": 0.4426,
      "step": 3340
    },
    {
      "epoch": 3.4862051015096305,
      "grad_norm": 1.4069997072219849,
      "learning_rate": 6.045833333333334e-05,
      "loss": 0.3872,
      "step": 3350
    },
    {
      "epoch": 3.4966163456533055,
      "grad_norm": 1.6938917636871338,
      "learning_rate": 6.004166666666667e-05,
      "loss": 0.402,
      "step": 3360
    },
    {
      "epoch": 3.507027589796981,
      "grad_norm": 1.5284597873687744,
      "learning_rate": 5.9625e-05,
      "loss": 0.3741,
      "step": 3370
    },
    {
      "epoch": 3.517438833940656,
      "grad_norm": 1.5214717388153076,
      "learning_rate": 5.9208333333333334e-05,
      "loss": 0.3826,
      "step": 3380
    },
    {
      "epoch": 3.527850078084331,
      "grad_norm": 1.3906747102737427,
      "learning_rate": 5.879166666666667e-05,
      "loss": 0.3894,
      "step": 3390
    },
    {
      "epoch": 3.5382613222280064,
      "grad_norm": 1.531187891960144,
      "learning_rate": 5.8375e-05,
      "loss": 0.4175,
      "step": 3400
    },
    {
      "epoch": 3.5486725663716814,
      "grad_norm": 0.9708847999572754,
      "learning_rate": 5.795833333333334e-05,
      "loss": 0.3006,
      "step": 3410
    },
    {
      "epoch": 3.5590838105153564,
      "grad_norm": 1.378161072731018,
      "learning_rate": 5.754166666666667e-05,
      "loss": 0.3868,
      "step": 3420
    },
    {
      "epoch": 3.569495054659032,
      "grad_norm": 1.3998488187789917,
      "learning_rate": 5.7125000000000006e-05,
      "loss": 0.386,
      "step": 3430
    },
    {
      "epoch": 3.579906298802707,
      "grad_norm": 1.6935404539108276,
      "learning_rate": 5.670833333333334e-05,
      "loss": 0.4206,
      "step": 3440
    },
    {
      "epoch": 3.590317542946382,
      "grad_norm": 1.158681035041809,
      "learning_rate": 5.629166666666666e-05,
      "loss": 0.3764,
      "step": 3450
    },
    {
      "epoch": 3.6007287870900573,
      "grad_norm": 1.4833017587661743,
      "learning_rate": 5.5875e-05,
      "loss": 0.362,
      "step": 3460
    },
    {
      "epoch": 3.6111400312337323,
      "grad_norm": 1.2227258682250977,
      "learning_rate": 5.545833333333334e-05,
      "loss": 0.3445,
      "step": 3470
    },
    {
      "epoch": 3.6215512753774077,
      "grad_norm": 1.4226853847503662,
      "learning_rate": 5.504166666666667e-05,
      "loss": 0.3655,
      "step": 3480
    },
    {
      "epoch": 3.6319625195210827,
      "grad_norm": 1.6241780519485474,
      "learning_rate": 5.4625000000000006e-05,
      "loss": 0.4055,
      "step": 3490
    },
    {
      "epoch": 3.642373763664758,
      "grad_norm": 1.4702223539352417,
      "learning_rate": 5.420833333333334e-05,
      "loss": 0.3444,
      "step": 3500
    },
    {
      "epoch": 3.652785007808433,
      "grad_norm": 1.2581937313079834,
      "learning_rate": 5.3791666666666675e-05,
      "loss": 0.3046,
      "step": 3510
    },
    {
      "epoch": 3.663196251952108,
      "grad_norm": 1.423309564590454,
      "learning_rate": 5.3374999999999996e-05,
      "loss": 0.419,
      "step": 3520
    },
    {
      "epoch": 3.6736074960957836,
      "grad_norm": 1.144249677658081,
      "learning_rate": 5.295833333333333e-05,
      "loss": 0.3394,
      "step": 3530
    },
    {
      "epoch": 3.6840187402394586,
      "grad_norm": 1.3282196521759033,
      "learning_rate": 5.2541666666666665e-05,
      "loss": 0.3264,
      "step": 3540
    },
    {
      "epoch": 3.6944299843831336,
      "grad_norm": 1.48269784450531,
      "learning_rate": 5.2125e-05,
      "loss": 0.4273,
      "step": 3550
    },
    {
      "epoch": 3.704841228526809,
      "grad_norm": 1.4880390167236328,
      "learning_rate": 5.1708333333333334e-05,
      "loss": 0.3461,
      "step": 3560
    },
    {
      "epoch": 3.715252472670484,
      "grad_norm": 1.4742732048034668,
      "learning_rate": 5.129166666666667e-05,
      "loss": 0.3519,
      "step": 3570
    },
    {
      "epoch": 3.725663716814159,
      "grad_norm": 1.2415069341659546,
      "learning_rate": 5.0875e-05,
      "loss": 0.3479,
      "step": 3580
    },
    {
      "epoch": 3.7360749609578345,
      "grad_norm": 1.7676537036895752,
      "learning_rate": 5.045833333333334e-05,
      "loss": 0.4357,
      "step": 3590
    },
    {
      "epoch": 3.7464862051015095,
      "grad_norm": 1.512398362159729,
      "learning_rate": 5.0041666666666666e-05,
      "loss": 0.4116,
      "step": 3600
    },
    {
      "epoch": 3.7568974492451845,
      "grad_norm": 1.5191041231155396,
      "learning_rate": 4.962500000000001e-05,
      "loss": 0.3799,
      "step": 3610
    },
    {
      "epoch": 3.76730869338886,
      "grad_norm": 1.4693044424057007,
      "learning_rate": 4.9208333333333335e-05,
      "loss": 0.3626,
      "step": 3620
    },
    {
      "epoch": 3.777719937532535,
      "grad_norm": 1.7466589212417603,
      "learning_rate": 4.879166666666667e-05,
      "loss": 0.3629,
      "step": 3630
    },
    {
      "epoch": 3.7881311816762104,
      "grad_norm": 1.5549168586730957,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 0.4037,
      "step": 3640
    },
    {
      "epoch": 3.7985424258198854,
      "grad_norm": 1.3073927164077759,
      "learning_rate": 4.795833333333333e-05,
      "loss": 0.3356,
      "step": 3650
    },
    {
      "epoch": 3.808953669963561,
      "grad_norm": 1.4033939838409424,
      "learning_rate": 4.7541666666666666e-05,
      "loss": 0.3651,
      "step": 3660
    },
    {
      "epoch": 3.819364914107236,
      "grad_norm": 1.3737943172454834,
      "learning_rate": 4.7125e-05,
      "loss": 0.346,
      "step": 3670
    },
    {
      "epoch": 3.829776158250911,
      "grad_norm": 1.1935148239135742,
      "learning_rate": 4.6708333333333335e-05,
      "loss": 0.3486,
      "step": 3680
    },
    {
      "epoch": 3.8401874023945863,
      "grad_norm": 1.278960108757019,
      "learning_rate": 4.629166666666667e-05,
      "loss": 0.4078,
      "step": 3690
    },
    {
      "epoch": 3.8505986465382613,
      "grad_norm": 1.2970024347305298,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 0.3341,
      "step": 3700
    },
    {
      "epoch": 3.8610098906819363,
      "grad_norm": 1.31894850730896,
      "learning_rate": 4.545833333333334e-05,
      "loss": 0.3793,
      "step": 3710
    },
    {
      "epoch": 3.871421134825612,
      "grad_norm": 1.0786874294281006,
      "learning_rate": 4.504166666666667e-05,
      "loss": 0.3435,
      "step": 3720
    },
    {
      "epoch": 3.881832378969287,
      "grad_norm": 1.6367247104644775,
      "learning_rate": 4.4625e-05,
      "loss": 0.4361,
      "step": 3730
    },
    {
      "epoch": 3.892243623112962,
      "grad_norm": 1.7811757326126099,
      "learning_rate": 4.4208333333333335e-05,
      "loss": 0.3499,
      "step": 3740
    },
    {
      "epoch": 3.9026548672566372,
      "grad_norm": 1.0646135807037354,
      "learning_rate": 4.379166666666667e-05,
      "loss": 0.4024,
      "step": 3750
    },
    {
      "epoch": 3.9130661114003122,
      "grad_norm": 1.3453110456466675,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 0.355,
      "step": 3760
    },
    {
      "epoch": 3.9234773555439872,
      "grad_norm": 1.4916131496429443,
      "learning_rate": 4.295833333333333e-05,
      "loss": 0.3206,
      "step": 3770
    },
    {
      "epoch": 3.9338885996876627,
      "grad_norm": 1.1576613187789917,
      "learning_rate": 4.2541666666666666e-05,
      "loss": 0.3306,
      "step": 3780
    },
    {
      "epoch": 3.9442998438313377,
      "grad_norm": 1.4937833547592163,
      "learning_rate": 4.2125e-05,
      "loss": 0.4052,
      "step": 3790
    },
    {
      "epoch": 3.954711087975013,
      "grad_norm": 1.4466501474380493,
      "learning_rate": 4.1708333333333335e-05,
      "loss": 0.3826,
      "step": 3800
    },
    {
      "epoch": 3.965122332118688,
      "grad_norm": 1.614989995956421,
      "learning_rate": 4.129166666666667e-05,
      "loss": 0.3506,
      "step": 3810
    },
    {
      "epoch": 3.9755335762623636,
      "grad_norm": 0.9251803755760193,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.3221,
      "step": 3820
    },
    {
      "epoch": 3.9859448204060386,
      "grad_norm": 1.4223129749298096,
      "learning_rate": 4.045833333333334e-05,
      "loss": 0.3925,
      "step": 3830
    },
    {
      "epoch": 3.9963560645497136,
      "grad_norm": 1.6546080112457275,
      "learning_rate": 4.0041666666666666e-05,
      "loss": 0.3947,
      "step": 3840
    },
    {
      "epoch": 4.006246746486205,
      "grad_norm": 0.9817076921463013,
      "learning_rate": 3.9625e-05,
      "loss": 0.3358,
      "step": 3850
    },
    {
      "epoch": 4.016657990629881,
      "grad_norm": 1.1802442073822021,
      "learning_rate": 3.9208333333333335e-05,
      "loss": 0.3118,
      "step": 3860
    },
    {
      "epoch": 4.027069234773555,
      "grad_norm": 1.0299533605575562,
      "learning_rate": 3.879166666666667e-05,
      "loss": 0.2944,
      "step": 3870
    },
    {
      "epoch": 4.037480478917231,
      "grad_norm": 1.2534483671188354,
      "learning_rate": 3.8375e-05,
      "loss": 0.3208,
      "step": 3880
    },
    {
      "epoch": 4.047891723060906,
      "grad_norm": 1.3644779920578003,
      "learning_rate": 3.795833333333333e-05,
      "loss": 0.3093,
      "step": 3890
    },
    {
      "epoch": 4.058302967204581,
      "grad_norm": 1.5324621200561523,
      "learning_rate": 3.754166666666667e-05,
      "loss": 0.3332,
      "step": 3900
    },
    {
      "epoch": 4.068714211348256,
      "grad_norm": 1.0778675079345703,
      "learning_rate": 3.7125e-05,
      "loss": 0.3419,
      "step": 3910
    },
    {
      "epoch": 4.079125455491932,
      "grad_norm": 1.1686310768127441,
      "learning_rate": 3.6708333333333336e-05,
      "loss": 0.3281,
      "step": 3920
    },
    {
      "epoch": 4.089536699635606,
      "grad_norm": 0.8417553901672363,
      "learning_rate": 3.629166666666667e-05,
      "loss": 0.3155,
      "step": 3930
    },
    {
      "epoch": 4.099947943779282,
      "grad_norm": 1.6325957775115967,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.3099,
      "step": 3940
    },
    {
      "epoch": 4.110359187922957,
      "grad_norm": 1.4513813257217407,
      "learning_rate": 3.545833333333333e-05,
      "loss": 0.3024,
      "step": 3950
    },
    {
      "epoch": 4.120770432066632,
      "grad_norm": 1.9323550462722778,
      "learning_rate": 3.504166666666667e-05,
      "loss": 0.2919,
      "step": 3960
    },
    {
      "epoch": 4.131181676210307,
      "grad_norm": 0.9185774326324463,
      "learning_rate": 3.4625e-05,
      "loss": 0.2934,
      "step": 3970
    },
    {
      "epoch": 4.1415929203539825,
      "grad_norm": 1.435032844543457,
      "learning_rate": 3.4208333333333336e-05,
      "loss": 0.3468,
      "step": 3980
    },
    {
      "epoch": 4.152004164497657,
      "grad_norm": 1.1638221740722656,
      "learning_rate": 3.3791666666666664e-05,
      "loss": 0.2814,
      "step": 3990
    },
    {
      "epoch": 4.1624154086413325,
      "grad_norm": 1.1949926614761353,
      "learning_rate": 3.3375e-05,
      "loss": 0.3271,
      "step": 4000
    },
    {
      "epoch": 4.172826652785008,
      "grad_norm": 1.3929316997528076,
      "learning_rate": 3.295833333333333e-05,
      "loss": 0.3198,
      "step": 4010
    },
    {
      "epoch": 4.1832378969286825,
      "grad_norm": 1.4499144554138184,
      "learning_rate": 3.254166666666667e-05,
      "loss": 0.3213,
      "step": 4020
    },
    {
      "epoch": 4.193649141072358,
      "grad_norm": 1.4506996870040894,
      "learning_rate": 3.2125e-05,
      "loss": 0.2909,
      "step": 4030
    },
    {
      "epoch": 4.204060385216033,
      "grad_norm": 1.1786906719207764,
      "learning_rate": 3.1708333333333336e-05,
      "loss": 0.3151,
      "step": 4040
    },
    {
      "epoch": 4.214471629359709,
      "grad_norm": 1.0680299997329712,
      "learning_rate": 3.129166666666667e-05,
      "loss": 0.3254,
      "step": 4050
    },
    {
      "epoch": 4.224882873503383,
      "grad_norm": 1.7653417587280273,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 0.2809,
      "step": 4060
    },
    {
      "epoch": 4.235294117647059,
      "grad_norm": 1.0570958852767944,
      "learning_rate": 3.0458333333333333e-05,
      "loss": 0.3094,
      "step": 4070
    },
    {
      "epoch": 4.245705361790734,
      "grad_norm": 1.317726731300354,
      "learning_rate": 3.0041666666666667e-05,
      "loss": 0.3088,
      "step": 4080
    },
    {
      "epoch": 4.256116605934409,
      "grad_norm": 1.5622293949127197,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 0.3075,
      "step": 4090
    },
    {
      "epoch": 4.266527850078084,
      "grad_norm": 1.0756704807281494,
      "learning_rate": 2.9208333333333333e-05,
      "loss": 0.42,
      "step": 4100
    },
    {
      "epoch": 4.27693909422176,
      "grad_norm": 1.225182056427002,
      "learning_rate": 2.8791666666666667e-05,
      "loss": 0.3251,
      "step": 4110
    },
    {
      "epoch": 4.287350338365434,
      "grad_norm": 1.15910804271698,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 0.2836,
      "step": 4120
    },
    {
      "epoch": 4.29776158250911,
      "grad_norm": 1.6229969263076782,
      "learning_rate": 2.7958333333333336e-05,
      "loss": 0.3701,
      "step": 4130
    },
    {
      "epoch": 4.308172826652785,
      "grad_norm": 1.4153337478637695,
      "learning_rate": 2.7541666666666664e-05,
      "loss": 0.2948,
      "step": 4140
    },
    {
      "epoch": 4.31858407079646,
      "grad_norm": 1.6373519897460938,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.3416,
      "step": 4150
    },
    {
      "epoch": 4.328995314940135,
      "grad_norm": 1.086486577987671,
      "learning_rate": 2.6708333333333337e-05,
      "loss": 0.2708,
      "step": 4160
    },
    {
      "epoch": 4.339406559083811,
      "grad_norm": 1.2327693700790405,
      "learning_rate": 2.629166666666667e-05,
      "loss": 0.2973,
      "step": 4170
    },
    {
      "epoch": 4.349817803227486,
      "grad_norm": 1.3499622344970703,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.3831,
      "step": 4180
    },
    {
      "epoch": 4.360229047371161,
      "grad_norm": 1.4768458604812622,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.346,
      "step": 4190
    },
    {
      "epoch": 4.370640291514836,
      "grad_norm": 1.2786173820495605,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.3105,
      "step": 4200
    },
    {
      "epoch": 4.381051535658512,
      "grad_norm": 1.2363959550857544,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.3221,
      "step": 4210
    },
    {
      "epoch": 4.391462779802186,
      "grad_norm": 1.0959805250167847,
      "learning_rate": 2.425e-05,
      "loss": 0.3287,
      "step": 4220
    },
    {
      "epoch": 4.401874023945862,
      "grad_norm": 1.2238205671310425,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.3131,
      "step": 4230
    },
    {
      "epoch": 4.412285268089537,
      "grad_norm": 1.6651300191879272,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.3012,
      "step": 4240
    },
    {
      "epoch": 4.422696512233212,
      "grad_norm": 1.6919056177139282,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.317,
      "step": 4250
    },
    {
      "epoch": 4.433107756376887,
      "grad_norm": 1.3312046527862549,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.2563,
      "step": 4260
    },
    {
      "epoch": 4.4435190005205625,
      "grad_norm": 1.528780460357666,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.3583,
      "step": 4270
    },
    {
      "epoch": 4.453930244664237,
      "grad_norm": 1.4035016298294067,
      "learning_rate": 2.175e-05,
      "loss": 0.313,
      "step": 4280
    },
    {
      "epoch": 4.4643414888079125,
      "grad_norm": 1.4102189540863037,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.3069,
      "step": 4290
    },
    {
      "epoch": 4.474752732951588,
      "grad_norm": 1.4256130456924438,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.3143,
      "step": 4300
    },
    {
      "epoch": 4.4851639770952625,
      "grad_norm": 1.6021697521209717,
      "learning_rate": 2.05e-05,
      "loss": 0.3284,
      "step": 4310
    },
    {
      "epoch": 4.495575221238938,
      "grad_norm": 2.0190203189849854,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.2581,
      "step": 4320
    },
    {
      "epoch": 4.505986465382613,
      "grad_norm": 1.953762173652649,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.3356,
      "step": 4330
    },
    {
      "epoch": 4.516397709526288,
      "grad_norm": 1.426108717918396,
      "learning_rate": 1.925e-05,
      "loss": 0.4108,
      "step": 4340
    },
    {
      "epoch": 4.526808953669963,
      "grad_norm": 1.2111941576004028,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.355,
      "step": 4350
    },
    {
      "epoch": 4.537220197813639,
      "grad_norm": 1.5450860261917114,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.3204,
      "step": 4360
    },
    {
      "epoch": 4.547631441957314,
      "grad_norm": 1.4250127077102661,
      "learning_rate": 1.8e-05,
      "loss": 0.3197,
      "step": 4370
    },
    {
      "epoch": 4.558042686100989,
      "grad_norm": 1.4922975301742554,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.3155,
      "step": 4380
    },
    {
      "epoch": 4.568453930244664,
      "grad_norm": 1.0844619274139404,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.2894,
      "step": 4390
    },
    {
      "epoch": 4.57886517438834,
      "grad_norm": 1.4881254434585571,
      "learning_rate": 1.675e-05,
      "loss": 0.3188,
      "step": 4400
    },
    {
      "epoch": 4.589276418532014,
      "grad_norm": 1.7706125974655151,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.3442,
      "step": 4410
    },
    {
      "epoch": 4.59968766267569,
      "grad_norm": 1.4234631061553955,
      "learning_rate": 1.5958333333333333e-05,
      "loss": 0.2894,
      "step": 4420
    },
    {
      "epoch": 4.610098906819365,
      "grad_norm": 1.6517211198806763,
      "learning_rate": 1.5541666666666667e-05,
      "loss": 0.3103,
      "step": 4430
    },
    {
      "epoch": 4.62051015096304,
      "grad_norm": 1.5700017213821411,
      "learning_rate": 1.5125e-05,
      "loss": 0.3612,
      "step": 4440
    },
    {
      "epoch": 4.630921395106715,
      "grad_norm": 1.416968822479248,
      "learning_rate": 1.4708333333333335e-05,
      "loss": 0.3268,
      "step": 4450
    },
    {
      "epoch": 4.641332639250391,
      "grad_norm": 1.4995349645614624,
      "learning_rate": 1.4291666666666667e-05,
      "loss": 0.2817,
      "step": 4460
    },
    {
      "epoch": 4.651743883394065,
      "grad_norm": 1.4362860918045044,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.2888,
      "step": 4470
    },
    {
      "epoch": 4.662155127537741,
      "grad_norm": 1.395496129989624,
      "learning_rate": 1.3458333333333335e-05,
      "loss": 0.2999,
      "step": 4480
    },
    {
      "epoch": 4.672566371681416,
      "grad_norm": 1.182092547416687,
      "learning_rate": 1.3041666666666666e-05,
      "loss": 0.2951,
      "step": 4490
    },
    {
      "epoch": 4.6829776158250915,
      "grad_norm": 1.5328582525253296,
      "learning_rate": 1.2625e-05,
      "loss": 0.3131,
      "step": 4500
    },
    {
      "epoch": 4.693388859968766,
      "grad_norm": 1.5030224323272705,
      "learning_rate": 1.2208333333333335e-05,
      "loss": 0.2939,
      "step": 4510
    },
    {
      "epoch": 4.7038001041124415,
      "grad_norm": 1.4077478647232056,
      "learning_rate": 1.1791666666666668e-05,
      "loss": 0.2988,
      "step": 4520
    },
    {
      "epoch": 4.714211348256117,
      "grad_norm": 1.4857187271118164,
      "learning_rate": 1.1375e-05,
      "loss": 0.3266,
      "step": 4530
    },
    {
      "epoch": 4.7246225923997915,
      "grad_norm": 1.390614628791809,
      "learning_rate": 1.0958333333333335e-05,
      "loss": 0.321,
      "step": 4540
    },
    {
      "epoch": 4.735033836543467,
      "grad_norm": 1.2039493322372437,
      "learning_rate": 1.0541666666666668e-05,
      "loss": 0.3293,
      "step": 4550
    },
    {
      "epoch": 4.745445080687142,
      "grad_norm": 1.0562418699264526,
      "learning_rate": 1.0125e-05,
      "loss": 0.3307,
      "step": 4560
    },
    {
      "epoch": 4.755856324830817,
      "grad_norm": 1.6065038442611694,
      "learning_rate": 9.708333333333333e-06,
      "loss": 0.3918,
      "step": 4570
    },
    {
      "epoch": 4.766267568974492,
      "grad_norm": 1.6667526960372925,
      "learning_rate": 9.291666666666666e-06,
      "loss": 0.3442,
      "step": 4580
    },
    {
      "epoch": 4.776678813118168,
      "grad_norm": 0.7264203429222107,
      "learning_rate": 8.875e-06,
      "loss": 0.2936,
      "step": 4590
    },
    {
      "epoch": 4.7870900572618424,
      "grad_norm": 1.6004571914672852,
      "learning_rate": 8.458333333333333e-06,
      "loss": 0.314,
      "step": 4600
    },
    {
      "epoch": 4.797501301405518,
      "grad_norm": 1.748022437095642,
      "learning_rate": 8.041666666666666e-06,
      "loss": 0.3397,
      "step": 4610
    },
    {
      "epoch": 4.807912545549193,
      "grad_norm": 0.9537254571914673,
      "learning_rate": 7.625e-06,
      "loss": 0.3236,
      "step": 4620
    },
    {
      "epoch": 4.818323789692869,
      "grad_norm": 1.3835694789886475,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 0.2676,
      "step": 4630
    },
    {
      "epoch": 4.828735033836543,
      "grad_norm": 1.5850036144256592,
      "learning_rate": 6.791666666666667e-06,
      "loss": 0.3421,
      "step": 4640
    },
    {
      "epoch": 4.839146277980219,
      "grad_norm": 1.4418290853500366,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.3293,
      "step": 4650
    },
    {
      "epoch": 4.849557522123893,
      "grad_norm": 1.3045309782028198,
      "learning_rate": 5.958333333333334e-06,
      "loss": 0.3153,
      "step": 4660
    },
    {
      "epoch": 4.859968766267569,
      "grad_norm": 1.1161632537841797,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.2747,
      "step": 4670
    },
    {
      "epoch": 4.870380010411244,
      "grad_norm": 1.403592586517334,
      "learning_rate": 5.125e-06,
      "loss": 0.3077,
      "step": 4680
    },
    {
      "epoch": 4.88079125455492,
      "grad_norm": 1.8877671957015991,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.34,
      "step": 4690
    },
    {
      "epoch": 4.891202498698594,
      "grad_norm": 1.6983568668365479,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 0.3221,
      "step": 4700
    }
  ],
  "logging_steps": 10,
  "max_steps": 4800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6295367575548723e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
